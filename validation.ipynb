{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LciKvMEzH9I-"
      },
      "outputs": [],
      "source": [
        "import sqlite3 as sql\n",
        "from contextlib import closing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from index import my_index\n",
        "from execute import my_execute"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sqlite3 does not support contextual closing natively (yet). Using a super-elegant workaround proposed by erlendaasland\\\n",
        "https://discuss.python.org/t/implicitly-close-sqlite3-connections-with-context-managers/33320/3"
      ],
      "metadata": {
        "id": "GGdRLObRKAlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_tran( db_name, query ):\n",
        "  with closing( sql.connect( db_name ) ) as conn:\n",
        "    cur = conn.execute( query )\n",
        "    cols = [ col[0] for col in cur.description ]\n",
        "    df = pd.DataFrame.from_records( cur, columns = cols )\n",
        "    return df\n",
        "\n",
        "db_name = \"secret.db\"\n",
        "get_gold_results = lambda query: safe_tran( db_name, query )"
      ],
      "metadata": {
        "id": "POoHIPqtIbVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_sqlite_query( clause ):\n",
        "  query = \"SELECT id FROM tbl WHERE \"\n",
        "  query += \" AND \".join( [ ' '.join( pred ) for pred in clause ] )\n",
        "  return query\n",
        "\n",
        "def eval_results( clause, disk, idx_stat ):\n",
        "  # Execute the query on an actual DB\n",
        "  df_gold = get_gold_results( make_sqlite_query( clause ) )\n",
        "\n",
        "  # Execute the query using the index and time it\n",
        "  tic = tm.perf_counter()\n",
        "  diskloc_list = my_execute( clause, idx_stat )\n",
        "  toc = tm.perf_counter()\n",
        "  t_idx = toc - tic\n",
        "\n",
        "  # Do sanity checks on the returned locations -- dont want any buffer overflow attacks :)\n",
        "  diskloc_list = np.minimum( np.maximum( diskloc_list, 0 ), len( disk ) - 1 )\n",
        "\n",
        "  # Find the seek and read time requried to retrieve records from the virtual disk\n",
        "  diffs = diskloc_list[ 1: ] - diskloc_list[ :-1 ]\n",
        "  # Take care of cases where we need to loop back to reach a record\n",
        "  diffs[ diffs <= 0 ] += len( disk )\n",
        "  t_seek = diffs.sum()\n",
        "  t_read = len( diskloc_list )\n",
        "  # Sanity check\n",
        "  assert( t_seek >= t_read - 1 )\n",
        "  t_seek -= t_read - 1\n",
        "  # Take care of pesky edge cases\n",
        "  if t_read == 0:\n",
        "    t_seek = 0\n",
        "\n",
        "  # Get hold of the tuples chosen by the index from the virtual disk\n",
        "  response_stu = []\n",
        "  if len( diskloc_list ) > 0:\n",
        "    response_stu = disk[ diskloc_list ]\n",
        "  df_stu = pd.DataFrame( response_stu, columns = [ \"id\" ] )\n",
        "\n",
        "  # Rename columns just to be safe so as to enable merging\n",
        "  df_stu.rename( dict( zip( df_stu.columns, df_gold.columns ) ), axis = 1, inplace = True )\n",
        "\n",
        "  union = pd.merge( df_gold, df_stu, how = \"outer\", indicator = True )\n",
        "  inter = pd.merge( df_gold, df_stu, how = \"inner\", indicator = True )\n",
        "\n",
        "  # If the gold response is not empty, use intersection over union score\n",
        "  # Since union removes duplicates, consider length of diskloc_list as well\n",
        "  if len( df_gold ) > 0:\n",
        "    score = round( len( inter ) / max( len( diskloc_list ), len( union ) ), 2 )\n",
        "  # If the gold response itself is empty, penalize non-empty response by index\n",
        "  elif len( df_gold ) == 0:\n",
        "    score = round( 1 / ( 1 + len( diskloc_list ) ), 2 )\n",
        "\n",
        "  return t_idx, t_seek, t_read, score"
      ],
      "metadata": {
        "id": "EY9Q-50DKTdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_trials = 3\n",
        "\n",
        "t_build = 0\n",
        "disk_size = np.int64(0)\n",
        "idx_size = 0\n",
        "t_idx = 0\n",
        "t_seek = np.int64(0)\n",
        "t_read = np.int64(0)\n",
        "score = 0"
      ],
      "metadata": {
        "id": "KeLLNTmoVB9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data to be indexed\n",
        "with open( \"secret.csv\", 'r' ) as csvfile:\n",
        "  reader = csv.reader( csvfile )\n",
        "  tuples = [ ( int( row[ 0 ] ), row[ 1 ], int( row[ 2 ] ) ) for row in reader ]\n",
        "\n",
        "# Create proper predicates out of CSV data\n",
        "def make_predicates( tok_list ):\n",
        "  if len( tok_list ) == 3:\n",
        "    return [ tok_list ]\n",
        "  if len( tok_list ) == 6:\n",
        "    return [ tok_list[ :3 ], tok_list[ 3: ] ]\n",
        "\n",
        "# Read the clauses that will constitute the evaluation queries\n",
        "with open( \"secret_clauses.csv\", 'r' ) as csvfile:\n",
        "  reader = csv.reader( csvfile )\n",
        "  c_list = [ make_predicates( row ) for row in reader ]"
      ],
      "metadata": {
        "id": "8heZOUbvWkbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range( n_trials ):\n",
        "  tic = tm.perf_counter()\n",
        "  disk, idx_stat = my_index( tuples )\n",
        "  disk = np.array( disk )\n",
        "  toc = tm.perf_counter()\n",
        "  t_build += toc - tic\n",
        "\n",
        "  disk_size += len( disk )\n",
        "\n",
        "  with open( f\"idx_dump_{t}.pkl\", \"wb\" ) as outfile:\n",
        "    pickle.dump( idx_stat, outfile, protocol=pickle.HIGHEST_PROTOCOL )\n",
        "\n",
        "  idx_size += os.path.getsize( f\"idx_dump_{t}.pkl\" )\n",
        "\n",
        "  for clause in c_list:\n",
        "    t_i, t_s, t_r, scr = eval_results( clause, disk, idx_stat )\n",
        "    t_idx += t_i\n",
        "    t_seek += t_s\n",
        "    t_read += t_r\n",
        "    score += scr"
      ],
      "metadata": {
        "id": "PiB-skm2VXFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_build /= n_trials\n",
        "disk_size /= n_trials\n",
        "idx_size /= n_trials\n",
        "t_idx /= n_trials\n",
        "t_seek /= n_trials\n",
        "t_read /= n_trials\n",
        "score /= n_trials\n",
        "score /= len( c_list )\n",
        "\n",
        "print( t_build, disk_size, idx_size, t_idx, t_seek, t_read, score )"
      ],
      "metadata": {
        "id": "tXvUu6PhW8p_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edae3aba-c5f4-4228-c019-8f81f4d6582f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12498304033334762 200000.0 869242.0 0.001000857333432729 0.0 7000000.0 0.14990000000000003\n"
          ]
        }
      ]
    }
  ]
}